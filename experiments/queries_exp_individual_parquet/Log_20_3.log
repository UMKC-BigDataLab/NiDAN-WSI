Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/04/27 10:40:40 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/04/27 10:40:55 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/04/27 10:40:55 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/04/27 10:40:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/04/27 10:40:58 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/04/27 10:41:13 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://128.110.152.45:4040
Spark context available as 'sc' (master = spark://ctl:7077, app id = app-20170427104058-0165).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/
         
Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.

scala> //spark-shell --master spark://ctl:7077 --driver-memory 28G  --executor-memory 28G  --executor-core s 8  --num-executors 16  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf s park.driver.maxResultSize=2g

scala> 

scala> 

scala> 

scala> 

scala> 

scala> import java.io.File
import java.io.File

scala> 

scala> import java.io.FileOutputStream
import java.io.FileOutputStream

scala> 

scala> 

scala> 

scala> val queryMsg = "#QUERY "
queryMsg: String = "#QUERY "

scala> 

scala> val loadDBMsg = "#LOAD_DB "
loadDBMsg: String = "#LOAD_DB "

scala> 

scala> val loadTable = "#LOAD_TABLE "
loadTable: String = "#LOAD_TABLE "

scala> 

scala> val loadSqlContext = "#LOAD_SQL_CONTEXT "
loadSqlContext: String = "#LOAD_SQL_CONTEXT "

scala> 

scala> val dataSource = "/nidan/parquet/slide20.prqt"
dataSource: String = /nidan/parquet/slide20.prqt

scala> 

scala> 

scala> 

scala> def show_timing[T](proc: => T): T = {
     | 
     |     val start=System.nanoTime()
     | 
     |     val res = proc
     | 
     |     val end = System.nanoTime()
     | 
     |     println("Time elapsed: " + (end-start)/1000000000.0 + " seconds")
     | 
     |     res
     | 
     | }
show_timing: [T](proc: => T)T

scala> 

scala> 

scala> 

scala> val writeToLocal = (in:(Array[Byte], Long, String)) =>{
     | 
     |     val bytes = in._1
     | 
     |     val output = in._3
     | 
     |     
     | 
     |     val writer = new FileOutputStream(output)
     | 
     |     writer.write(bytes)
     | 
     |     writer.close
     | 
     |   }
writeToLocal: ((Array[Byte], Long, String)) => Unit = <function1>

scala> 

scala> 

scala> 

scala> val queries = List(
     | 
     | ("SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=1 and imageLevel = 0  and imageId = '20.svs'",1),
     | 
     | ("SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=2 and imageLevel = 0  and imageId = '20.svs'",2),
     | 
     | ("SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=4 and imageLevel = 0  and imageId = '20.svs'",4),
     | 
     | ("SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=8 and imageLevel = 0  and imageId = '20.svs'",8)
     | 
     | )
queries: List[(String, Int)] = List((SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=1 and imageLevel = 0 and imageId = '20.svs',1), (SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=2 and imageLevel = 0 and imageId = '20.svs',2), (SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=4 and imageLevel = 0 and imageId = '20.svs',4), (SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=8 and imageLevel = 0 and imageId = '20.svs',8))

scala> 

scala> 

scala> 

scala> 

scala> 

scala> val sqlContext = show_timing{new org.apache.spark.sql.SQLContext(sc)}
warning: there was one deprecation warning; re-run with -deprecation for details
Time elapsed: 0.001277662 seconds
sqlContext: org.apache.spark.sql.SQLContext = org.apache.spark.sql.SQLContext@664217a8

scala> 

scala> val pf = show_timing{sqlContext.read.parquet(dataSource).createOrReplaceTempView("data")}
Time elapsed: 9.114790438 seconds
pf: Unit = ()

scala> 

scala> 

scala> 

scala> show_timing{sqlContext.sql(queries(0)._1).map(_.getAs[Array[Byte]](0)).rdd.zipWithIndex.map{case (b ytes, index) => (bytes, index, s"o6_${index}.JPEG")}.collect.map(writeToLocal).filter(_ => false).size}
[Stage 1:>                                                      (0 + 104) / 109][Stage 1:====>                                                  (8 + 101) / 109][Stage 1:====>                                                  (9 + 100) / 109][Stage 1:=====>                                                 (10 + 99) / 109][Stage 1:=====>                                                 (11 + 98) / 109][Stage 1:======>                                                (13 + 96) / 109][Stage 1:============================>                          (57 + 52) / 109][Stage 1:=====================================================> (107 + 2) / 109][Stage 1:======================================================>(108 + 1) / 109]                                                                                [Stage 2:>                                                      (0 + 104) / 110][Stage 2:====>                                                 (10 + 100) / 110][Stage 2:=================>                                     (35 + 75) / 110][Stage 2:===============================>                       (62 + 48) / 110][Stage 2:==========================================>            (84 + 26) / 110][Stage 2:=================================================>     (98 + 12) / 110][Stage 2:====================================================>  (105 + 5) / 110][Stage 2:=====================================================> (106 + 4) / 110][Stage 2:=====================================================> (107 + 3) / 110][Stage 2:======================================================>(109 + 1) / 110]                                                                                Time elapsed: 13.811619952 seconds
res0: Int = 0

scala> 

scala> 

scala> 

scala> for (query <- queries){
     | 
     | println(s">> Running query: ${query._1}")
     | 
     | show_timing{sqlContext.sql(query._1).map(_.getAs[Array[Byte]](0)).rdd.zipWithIndex.map{case (bytes,  index) => (bytes, index, s"o6_${index}.JPEG")}.collect.map(writeToLocal).filter(_ => false).size}
     | 
     | }
>> Running query: SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=1 and imageLevel = 0 and imageId = '20.svs'
[Stage 3:>                                                      (0 + 104) / 109][Stage 3:==>                                                    (5 + 104) / 109][Stage 3:=========>                                             (19 + 90) / 109][Stage 3:====================>                                  (41 + 68) / 109][Stage 3:================================>                      (65 + 44) / 109][Stage 3:========================================>              (81 + 28) / 109][Stage 3:=================================================>     (99 + 10) / 109][Stage 3:====================================================>  (105 + 4) / 109][Stage 3:=====================================================> (106 + 3) / 109]                                                                                [Stage 4:=>                                                     (2 + 104) / 110][Stage 4:====>                                                  (9 + 101) / 110][Stage 4:=================>                                     (35 + 75) / 110][Stage 4:===============================>                       (63 + 47) / 110][Stage 4:==========================================>            (84 + 26) / 110][Stage 4:=================================================>     (99 + 11) / 110][Stage 4:====================================================>  (104 + 6) / 110][Stage 4:=====================================================> (106 + 4) / 110][Stage 4:=====================================================> (107 + 3) / 110][Stage 4:======================================================>(108 + 2) / 110][Stage 4:======================================================>(109 + 1) / 110]                                                                                Time elapsed: 6.971438132 seconds
>> Running query: SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=2 and imageLevel = 0 and imageId = '20.svs'
[Stage 5:>                                                      (0 + 104) / 109][Stage 5:====>                                                  (9 + 100) / 109][Stage 5:============>                                          (24 + 85) / 109][Stage 5:============================>                          (57 + 52) / 109][Stage 5:==========================================>            (84 + 25) / 109][Stage 5:=================================================>     (98 + 11) / 109][Stage 5:====================================================>  (105 + 4) / 109][Stage 5:=====================================================> (106 + 3) / 109][Stage 5:=====================================================> (107 + 2) / 109][Stage 5:======================================================>(108 + 1) / 109]                                                                                [Stage 6:>                                                      (0 + 104) / 110][Stage 6:========>                                              (17 + 93) / 110][Stage 6:==================>                                    (36 + 74) / 110][Stage 6:==================================>                    (68 + 42) / 110][Stage 6:=========================================>             (83 + 27) / 110][Stage 6:================================================>      (96 + 14) / 110][Stage 6:=====================================================> (106 + 4) / 110][Stage 6:======================================================>(108 + 2) / 110][Stage 6:======================================================>(109 + 1) / 110]                                                                                Time elapsed: 6.771696472 seconds
>> Running query: SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=4 and imageLevel = 0 and imageId = '20.svs'
[Stage 7:>                                                      (0 + 104) / 109][Stage 7:====>                                                  (8 + 101) / 109][Stage 7:===================>                                   (39 + 70) / 109][Stage 7:==============================>                        (61 + 48) / 109][Stage 7:===========================================>           (86 + 23) / 109][Stage 7:================================================>      (97 + 12) / 109][Stage 7:====================================================>  (104 + 5) / 109][Stage 7:=====================================================> (106 + 3) / 109][Stage 7:======================================================>(108 + 1) / 109]                                                                                [Stage 8:>                                                      (0 + 104) / 110][Stage 8:===>                                                   (7 + 103) / 110][Stage 8:==============>                                        (29 + 81) / 110][Stage 8:=============================>                         (58 + 52) / 110][Stage 8:======================================>                (77 + 33) / 110][Stage 8:================================================>      (96 + 14) / 110][Stage 8:===================================================>   (103 + 7) / 110][Stage 8:====================================================>  (105 + 5) / 110][Stage 8:=====================================================> (106 + 4) / 110][Stage 8:=====================================================> (107 + 3) / 110][Stage 8:======================================================>(108 + 2) / 110][Stage 8:======================================================>(109 + 1) / 110]                                                                                Time elapsed: 7.120113036 seconds
>> Running query: SELECT imageBytes from data where partitionZIndex >= 1 and partitionZIndex <=8 and imageLevel = 0 and imageId = '20.svs'
[Stage 9:>                                                        (0 + 0) / 109][Stage 9:>                                                      (0 + 104) / 109][Stage 9:====>                                                  (8 + 101) / 109][Stage 9:===========>                                           (23 + 86) / 109][Stage 9:=============================>                         (58 + 51) / 109][Stage 9:============================================>          (89 + 20) / 109][Stage 9:================================================>      (97 + 12) / 109][Stage 9:===================================================>   (102 + 7) / 109][Stage 9:===================================================>   (103 + 6) / 109][Stage 9:=====================================================> (106 + 3) / 109][Stage 9:=====================================================> (107 + 2) / 109][Stage 9:======================================================>(108 + 1) / 109]                                                                                [Stage 10:>                                                     (0 + 104) / 110][Stage 10:=>                                                    (4 + 104) / 110][Stage 10:=========>                                            (20 + 90) / 110][Stage 10:====================>                                 (41 + 69) / 110][Stage 10:================================>                     (67 + 43) / 110][Stage 10:===========================================>          (88 + 22) / 110][Stage 10:================================================>     (98 + 12) / 110][Stage 10:==================================================>   (102 + 8) / 110][Stage 10:===================================================>  (104 + 6) / 110][Stage 10:====================================================> (106 + 4) / 110][Stage 10:=====================================================>(108 + 2) / 110][Stage 10:=====================================================>(109 + 1) / 110]                                                                                Time elapsed: 10.224014051 seconds

scala> 

scala> 

scala> 

scala> 

scala> 

scala> :quit
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
17/04/27 10:42:30 ERROR cluster.StandaloneSchedulerBackend: Could not find CoarseGrainedScheduler.
org.apache.spark.SparkException: Could not find CoarseGrainedScheduler.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:154)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:129)
	at org.apache.spark.rpc.netty.NettyRpcEnv.ask(NettyRpcEnv.scala:225)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:507)
	at org.apache.spark.rpc.RpcEndpointRef.ask(RpcEndpointRef.scala:63)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.removeExecutor(CoarseGrainedSchedulerBackend.scala:423)
	at org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.executorRemoved(StandaloneSchedulerBackend.scala:160)
	at org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:182)
	at org.apache.spark.rpc.netty.Inbox$$anonfun$process$1.apply$mcV$sp(Inbox.scala:117)
	at org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:205)
	at org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:101)
	at org.apache.spark.rpc.netty.Dispatcher$MessageLoop.run(Dispatcher.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
