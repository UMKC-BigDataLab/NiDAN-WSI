>> Changing the name of the Slides, from slide7 to slide91
>> Changing the name of all the individual files in slide91
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide91 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:53:09 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:09 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:53:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:53:11 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:53:11 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:53:11 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:53:11 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:53:11 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:53:11 INFO util.Utils: Successfully started service 'sparkDriver' on port 44046.
17/05/30 12:53:11 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:53:11 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:53:11 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:53:11 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:53:11 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-797847e1-063a-40cc-8a5f-b27914b1a02b
17/05/30 12:53:11 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:53:11 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:53:12 INFO util.log: Logging initialized @3681ms
17/05/30 12:53:12 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:53:12 INFO server.ServerConnector: Started ServerConnector@697eb991{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:53:12 INFO server.Server: Started @3945ms
17/05/30 12:53:12 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:53:12 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:53:12 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:44046/jars/nidan.core-assembly-0.1.jar with timestamp 1496170392419
17/05/30 12:53:12 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:53:12 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39590.
17/05/30 12:53:12 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:39590
17/05/30 12:53:12 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:53:12 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 39590, None)
17/05/30 12:53:12 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:39590 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 39590, None)
17/05/30 12:53:12 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 39590, None)
17/05/30 12:53:12 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 39590, None)
17/05/30 12:53:12 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75504cef{/metrics/json,null,AVAILABLE}
17/05/30 12:53:12 WARN root: >>> -11
17/05/30 12:53:12 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:12 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/05/30 12:53:13 WARN root: >> Using 2 partitions in the system
17/05/30 12:53:13 WARN root: >> Generating the SortRDD
17/05/30 12:53:15 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:15 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:15 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:15 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:53:15 WARN root: >> SortRDD has been generated with option ZINDEX
17/05/30 12:53:15 WARN root: >> Saving memory by unpersisting RDDs
17/05/30 12:53:15 WARN root: >> Starting to generate the Parquet file
17/05/30 12:53:15 WARN root: >> Storing using GROUP option
17/05/30 12:53:23 WARN datasources.PartitioningAwareFileIndex: The directory hdfs://ctl:9000/nidan/orc/individualORC/slide91 was not found. Was it deleted very recently?
17/05/30 12:53:25 WARN scheduler.TaskSetManager: Stage 3 contains a task of very large size (120549 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:29 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 8/274
17/05/30 12:53:32 WARN scheduler.TaskSetManager: Stage 4 contains a task of very large size (108299 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:34 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 14/274
17/05/30 12:53:36 WARN scheduler.TaskSetManager: Stage 5 contains a task of very large size (123699 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:38 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 20/274
17/05/30 12:53:41 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (121188 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:42 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 26/274
17/05/30 12:53:45 WARN scheduler.TaskSetManager: Stage 7 contains a task of very large size (111158 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 31/274
17/05/30 12:53:48 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (121058 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:50 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 37/274
17/05/30 12:53:53 WARN scheduler.TaskSetManager: Stage 9 contains a task of very large size (113724 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:54 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 42/274
17/05/30 12:53:57 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (110810 KB). The maximum recommended task size is 100 KB.
17/05/30 12:53:58 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 46/274
17/05/30 12:54:00 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (115497 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:02 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 50/274
17/05/30 12:54:04 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (125437 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:06 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 55/274
17/05/30 12:54:09 WARN scheduler.TaskSetManager: Stage 13 contains a task of very large size (129415 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:10 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 59/274
17/05/30 12:54:13 WARN scheduler.TaskSetManager: Stage 14 contains a task of very large size (107056 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:13 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 62/274
17/05/30 12:54:17 WARN scheduler.TaskSetManager: Stage 15 contains a task of very large size (125647 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:19 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 67/274
17/05/30 12:54:22 WARN scheduler.TaskSetManager: Stage 16 contains a task of very large size (114005 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:23 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 73/274
17/05/30 12:54:26 WARN scheduler.TaskSetManager: Stage 17 contains a task of very large size (107583 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:28 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 78/274
17/05/30 12:54:30 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (121243 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:32 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 84/274
17/05/30 12:54:36 WARN scheduler.TaskSetManager: Stage 19 contains a task of very large size (118664 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:37 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 91/274
17/05/30 12:54:41 WARN scheduler.TaskSetManager: Stage 20 contains a task of very large size (110826 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:43 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 97/274
17/05/30 12:54:45 WARN scheduler.TaskSetManager: Stage 21 contains a task of very large size (103566 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 100/274
17/05/30 12:54:48 WARN scheduler.TaskSetManager: Stage 22 contains a task of very large size (111927 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:49 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 103/274
17/05/30 12:54:51 WARN scheduler.TaskSetManager: Stage 23 contains a task of very large size (106160 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 106/274
17/05/30 12:54:55 WARN scheduler.TaskSetManager: Stage 24 contains a task of very large size (104617 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:56 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 109/274
17/05/30 12:54:58 WARN scheduler.TaskSetManager: Stage 25 contains a task of very large size (110447 KB). The maximum recommended task size is 100 KB.
17/05/30 12:54:59 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 112/274
17/05/30 12:55:02 WARN scheduler.TaskSetManager: Stage 26 contains a task of very large size (100868 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:03 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 115/274
17/05/30 12:55:06 WARN scheduler.TaskSetManager: Stage 27 contains a task of very large size (110707 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:07 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 120/274
17/05/30 12:55:09 WARN scheduler.TaskSetManager: Stage 28 contains a task of very large size (107255 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:10 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 123/274
17/05/30 12:55:14 WARN scheduler.TaskSetManager: Stage 29 contains a task of very large size (126779 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:15 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 128/274
17/05/30 12:55:18 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (116709 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 132/274
17/05/30 12:55:21 WARN scheduler.TaskSetManager: Stage 31 contains a task of very large size (102096 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:22 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 135/274
17/05/30 12:55:25 WARN scheduler.TaskSetManager: Stage 32 contains a task of very large size (117791 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:26 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 139/274
17/05/30 12:55:29 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (104080 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:30 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 144/274
17/05/30 12:55:33 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (101458 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:34 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 147/274
17/05/30 12:55:37 WARN scheduler.TaskSetManager: Stage 35 contains a task of very large size (106212 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:38 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 150/274
17/05/30 12:55:40 WARN scheduler.TaskSetManager: Stage 36 contains a task of very large size (126442 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:42 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 154/274
17/05/30 12:55:45 WARN scheduler.TaskSetManager: Stage 37 contains a task of very large size (118285 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 158/274
17/05/30 12:55:49 WARN scheduler.TaskSetManager: Stage 38 contains a task of very large size (129431 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:51 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 163/274
17/05/30 12:55:54 WARN scheduler.TaskSetManager: Stage 39 contains a task of very large size (113598 KB). The maximum recommended task size is 100 KB.
17/05/30 12:55:55 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 169/274
17/05/30 12:55:58 WARN scheduler.TaskSetManager: Stage 40 contains a task of very large size (125769 KB). The maximum recommended task size is 100 KB.
17/05/30 12:56:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide91 Progress: 175/274
Exception in thread "main" java.io.IOException: Input/output error
	at sun.nio.ch.FileDispatcherImpl.read0(Native Method)
	at sun.nio.ch.FileDispatcherImpl.read(FileDispatcherImpl.java:46)
	at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:223)
	at sun.nio.ch.IOUtil.read(IOUtil.java:197)
	at sun.nio.ch.FileChannelImpl.read(FileChannelImpl.java:159)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:65)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:109)
	at sun.nio.ch.ChannelInputStream.read(ChannelInputStream.java:103)
	at java.nio.file.Files.read(Files.java:3105)
	at java.nio.file.Files.readAllBytes(Files.java:3158)
	at nidan.main.MainSparkSQL$$anonfun$12.apply(MainSparkSQL.scala:322)
	at nidan.main.MainSparkSQL$$anonfun$12.apply(MainSparkSQL.scala:321)
	at nidan.main.MainSparkSQL$$anonfun$processDataSlidesGrouped$1.apply(MainSparkSQL.scala:628)
	at nidan.main.MainSparkSQL$$anonfun$processDataSlidesGrouped$1.apply(MainSparkSQL.scala:625)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)
	at nidan.main.MainSparkSQL$.processDataSlidesGrouped(MainSparkSQL.scala:625)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:597)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	3m7.295s
user	5m39.940s
sys	0m17.256s
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide7': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide7': Input/output error
>> Changing the name of all the individual files in slide7
>> Changing the name of the Slides, from slide1 to slide92
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide92': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide92/*.JPEG': Input/output error
>> Changing the name of all the individual files in slide92
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide92/*.JPEG': Input/output error
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide92 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:56:17 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:17 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:56:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:56:18 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:56:18 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:56:18 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:56:18 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:56:18 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:56:19 INFO util.Utils: Successfully started service 'sparkDriver' on port 43073.
17/05/30 12:56:19 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:56:19 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:56:19 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:56:19 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:56:19 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-082a69c3-a6c3-4c01-9709-0d29b76926ce
17/05/30 12:56:19 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:56:19 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:56:19 INFO util.log: Logging initialized @4012ms
17/05/30 12:56:19 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:56:20 INFO server.ServerConnector: Started ServerConnector@69f63d95{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:56:20 INFO server.Server: Started @4313ms
17/05/30 12:56:20 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:56:20 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:56:20 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:43073/jars/nidan.core-assembly-0.1.jar with timestamp 1496170580133
17/05/30 12:56:20 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:56:20 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36678.
17/05/30 12:56:20 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:36678
17/05/30 12:56:20 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:56:20 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 36678, None)
17/05/30 12:56:20 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:36678 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 36678, None)
17/05/30 12:56:20 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 36678, None)
17/05/30 12:56:20 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 36678, None)
17/05/30 12:56:20 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c8a68c1{/metrics/json,null,AVAILABLE}
17/05/30 12:56:20 WARN root: >>> -11
17/05/30 12:56:20 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:20 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
Exception in thread "main" java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.SeqLike$class.size(SeqLike.scala:106)
	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186)
	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22)
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at nidan.main.MainOpenSlide$.getMappingIOFiles1(MainOpenSlide.scala:110)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:567)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	0m5.579s
user	0m11.348s
sys	0m0.452s
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide1': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide1': Input/output error
>> Changing the name of all the individual files in slide1
>> Changing the name of the Slides, from slide2 to slide93
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide93': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide93/*.JPEG': Input/output error
>> Changing the name of all the individual files in slide93
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide93/*.JPEG': Input/output error
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide93 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:56:22 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:22 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:56:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:56:24 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:56:24 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:56:24 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:56:24 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:56:24 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:56:24 INFO util.Utils: Successfully started service 'sparkDriver' on port 36349.
17/05/30 12:56:24 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:56:24 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:56:24 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:56:24 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:56:25 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-029a85a4-624f-4f35-812d-b05f2e01e8a6
17/05/30 12:56:25 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:56:25 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:56:25 INFO util.log: Logging initialized @4046ms
17/05/30 12:56:25 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/jobs/job,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/jobs/job/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/stage,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/stage/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/stages/pool,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/stages/pool/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/storage/rdd,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/environment,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/environment/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/executors/threadDump,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/static,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/api,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:56:25 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:56:25 INFO server.ServerConnector: Started ServerConnector@68b52649{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:56:25 INFO server.Server: Started @4342ms
17/05/30 12:56:25 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:56:25 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:56:25 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:36349/jars/nidan.core-assembly-0.1.jar with timestamp 1496170585779
17/05/30 12:56:25 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:56:25 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33025.
17/05/30 12:56:25 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:33025
17/05/30 12:56:25 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:56:25 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 33025, None)
17/05/30 12:56:25 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:33025 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 33025, None)
17/05/30 12:56:25 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 33025, None)
17/05/30 12:56:25 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 33025, None)
17/05/30 12:56:26 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56193c7d{/metrics/json,null,AVAILABLE}
17/05/30 12:56:26 WARN root: >>> -11
17/05/30 12:56:26 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:26 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
Exception in thread "main" java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.SeqLike$class.size(SeqLike.scala:106)
	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186)
	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22)
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at nidan.main.MainOpenSlide$.getMappingIOFiles1(MainOpenSlide.scala:110)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:567)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	0m6.169s
user	0m11.724s
sys	0m0.440s
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide2': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide2': Input/output error
>> Changing the name of all the individual files in slide2
>> Changing the name of the Slides, from slide3 to slide94
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide94': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide94/*.JPEG': Input/output error
>> Changing the name of all the individual files in slide94
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide94/*.JPEG': Input/output error
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide94 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:56:29 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:29 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:56:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:56:30 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:56:30 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:56:30 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:56:30 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:56:30 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:56:31 INFO util.Utils: Successfully started service 'sparkDriver' on port 38089.
17/05/30 12:56:31 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:56:31 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:56:31 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:56:31 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:56:31 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-b20c36da-fc71-4c25-91d9-e3d332903d5b
17/05/30 12:56:31 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:56:31 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:56:31 INFO util.log: Logging initialized @4216ms
17/05/30 12:56:31 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:56:32 INFO server.ServerConnector: Started ServerConnector@69f63d95{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:56:32 INFO server.Server: Started @4534ms
17/05/30 12:56:32 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:56:32 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:56:32 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:38089/jars/nidan.core-assembly-0.1.jar with timestamp 1496170592149
17/05/30 12:56:32 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:56:32 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45929.
17/05/30 12:56:32 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:45929
17/05/30 12:56:32 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:56:32 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 45929, None)
17/05/30 12:56:32 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:45929 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 45929, None)
17/05/30 12:56:32 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 45929, None)
17/05/30 12:56:32 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 45929, None)
17/05/30 12:56:32 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c8a68c1{/metrics/json,null,AVAILABLE}
17/05/30 12:56:32 WARN root: >>> -11
17/05/30 12:56:32 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:32 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
Exception in thread "main" java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.SeqLike$class.size(SeqLike.scala:106)
	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186)
	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22)
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at nidan.main.MainOpenSlide$.getMappingIOFiles1(MainOpenSlide.scala:110)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:567)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	0m6.082s
user	0m12.364s
sys	0m0.388s
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide3': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide3': Input/output error
>> Changing the name of all the individual files in slide3
>> Changing the name of the Slides, from slide4 to slide95
mv: failed to access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide95': Input/output error
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide95/*.JPEG': Input/output error
>> Changing the name of all the individual files in slide95
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide95/*.JPEG': Input/output error
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide95 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:56:36 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:36 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:56:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:56:37 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:56:37 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:56:37 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:56:37 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:56:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:56:38 INFO util.Utils: Successfully started service 'sparkDriver' on port 44995.
17/05/30 12:56:38 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:56:38 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:56:38 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:56:38 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:56:38 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-e760a64e-01cc-4f00-925e-b094e5885baa
17/05/30 12:56:38 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:56:38 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:56:38 INFO util.log: Logging initialized @4084ms
17/05/30 12:56:39 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/jobs/job,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/jobs/job/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/stage,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/stage/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/stages/pool,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/stages/pool/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/storage/rdd,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/environment,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/environment/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/executors/threadDump,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/static,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/api,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:56:39 INFO server.ServerConnector: Started ServerConnector@356b42f{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:56:39 INFO server.Server: Started @4403ms
17/05/30 12:56:39 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:56:39 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:56:39 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:44995/jars/nidan.core-assembly-0.1.jar with timestamp 1496170599352
17/05/30 12:56:39 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:56:39 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44776.
17/05/30 12:56:39 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:44776
17/05/30 12:56:39 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:56:39 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 44776, None)
17/05/30 12:56:39 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:44776 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 44776, None)
17/05/30 12:56:39 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 44776, None)
17/05/30 12:56:39 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 44776, None)
17/05/30 12:56:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56193c7d{/metrics/json,null,AVAILABLE}
17/05/30 12:56:39 WARN root: >>> -11
17/05/30 12:56:39 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:39 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
Exception in thread "main" java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.SeqLike$class.size(SeqLike.scala:106)
	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186)
	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22)
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at nidan.main.MainOpenSlide$.getMappingIOFiles1(MainOpenSlide.scala:110)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:567)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	0m6.219s
user	0m12.752s
sys	0m0.452s
mv: cannot stat '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide95': No such file or directory
>> Changing the name of all the individual files in slide4
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_1_0_0_0_12201_8870_0_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_1_0_0_0_12201_8870_0_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_10_3_1_1_24402_17740_12201_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_10_3_1_1_24402_17740_12201_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_11_6_1_2_36603_17740_24402_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_11_6_1_2_36603_17740_24402_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_12_7_1_3_48804_17740_36603_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_12_7_1_3_48804_17740_36603_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_13_18_1_4_61005_17740_48804_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_13_18_1_4_61005_17740_48804_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_14_19_1_5_73206_17740_61005_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_14_19_1_5_73206_17740_61005_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_15_22_1_6_85407_17740_73206_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_15_22_1_6_85407_17740_73206_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_16_23_1_7_97608_17740_85407_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_16_23_1_7_97608_17740_85407_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_17_8_2_0_12201_26610_0_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_17_8_2_0_12201_26610_0_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_18_9_2_1_24402_26610_12201_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_18_9_2_1_24402_26610_12201_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_19_12_2_2_36603_26610_24402_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_19_12_2_2_36603_26610_24402_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_20_13_2_3_48804_26610_36603_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_20_13_2_3_48804_26610_36603_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_2_1_0_1_24402_8870_12201_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_2_1_0_1_24402_8870_12201_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_21_24_2_4_61005_26610_48804_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_21_24_2_4_61005_26610_48804_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_22_25_2_5_73206_26610_61005_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_22_25_2_5_73206_26610_61005_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_23_28_2_6_85407_26610_73206_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_23_28_2_6_85407_26610_73206_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_24_29_2_7_97608_26610_85407_17740.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_24_29_2_7_97608_26610_85407_17740.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_25_10_3_0_12201_35480_0_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_25_10_3_0_12201_35480_0_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_26_11_3_1_24402_35480_12201_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_26_11_3_1_24402_35480_12201_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_27_14_3_2_36603_35480_24402_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_27_14_3_2_36603_35480_24402_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_28_15_3_3_48804_35480_36603_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_28_15_3_3_48804_35480_36603_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_29_26_3_4_61005_35480_48804_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_29_26_3_4_61005_35480_48804_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_30_27_3_5_73206_35480_61005_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_30_27_3_5_73206_35480_61005_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_31_30_3_6_85407_35480_73206_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_31_30_3_6_85407_35480_73206_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_32_31_3_7_97608_35480_85407_26610.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_32_31_3_7_97608_35480_85407_26610.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_33_32_4_0_12201_44350_0_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_33_32_4_0_12201_44350_0_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_3_4_0_2_36603_8870_24402_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_3_4_0_2_36603_8870_24402_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_34_33_4_1_24402_44350_12201_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_34_33_4_1_24402_44350_12201_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_35_36_4_2_36603_44350_24402_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_35_36_4_2_36603_44350_24402_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_36_37_4_3_48804_44350_36603_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_36_37_4_3_48804_44350_36603_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_37_48_4_4_61005_44350_48804_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_37_48_4_4_61005_44350_48804_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_38_49_4_5_73206_44350_61005_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_38_49_4_5_73206_44350_61005_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_39_52_4_6_85407_44350_73206_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_39_52_4_6_85407_44350_73206_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_40_53_4_7_97608_44350_85407_35480.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_40_53_4_7_97608_44350_85407_35480.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_41_34_5_0_12201_53220_0_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_41_34_5_0_12201_53220_0_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_42_35_5_1_24402_53220_12201_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_42_35_5_1_24402_53220_12201_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_43_38_5_2_36603_53220_24402_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_43_38_5_2_36603_53220_24402_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_44_39_5_3_48804_53220_36603_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_44_39_5_3_48804_53220_36603_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_4_5_0_3_48804_8870_36603_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_4_5_0_3_48804_8870_36603_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_45_50_5_4_61005_53220_48804_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_45_50_5_4_61005_53220_48804_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_46_51_5_5_73206_53220_61005_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_46_51_5_5_73206_53220_61005_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_47_54_5_6_85407_53220_73206_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_47_54_5_6_85407_53220_73206_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_48_55_5_7_97608_53220_85407_44350.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_48_55_5_7_97608_53220_85407_44350.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_49_40_6_0_12201_62090_0_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_49_40_6_0_12201_62090_0_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_50_41_6_1_24402_62090_12201_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_50_41_6_1_24402_62090_12201_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_51_44_6_2_36603_62090_24402_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_51_44_6_2_36603_62090_24402_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_5_16_0_4_61005_8870_48804_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_5_16_0_4_61005_8870_48804_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_52_45_6_3_48804_62090_36603_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_52_45_6_3_48804_62090_36603_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_53_56_6_4_61005_62090_48804_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_53_56_6_4_61005_62090_48804_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_54_57_6_5_73206_62090_61005_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_54_57_6_5_73206_62090_61005_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_55_60_6_6_85407_62090_73206_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_55_60_6_6_85407_62090_73206_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_56_61_6_7_97608_62090_85407_53220.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_56_61_6_7_97608_62090_85407_53220.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_57_42_7_0_12201_70964_0_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_57_42_7_0_12201_70964_0_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_58_43_7_1_24402_70964_12201_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_58_43_7_1_24402_70964_12201_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_59_46_7_2_36603_70964_24402_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_59_46_7_2_36603_70964_24402_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_60_47_7_3_48804_70964_36603_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_60_47_7_3_48804_70964_36603_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_61_58_7_4_61005_70964_48804_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_61_58_7_4_61005_70964_48804_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_6_17_0_5_73206_8870_61005_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_6_17_0_5_73206_8870_61005_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_62_59_7_5_73206_70964_61005_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_62_59_7_5_73206_70964_61005_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_63_62_7_6_85407_70964_73206_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_63_62_7_6_85407_70964_73206_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_64_63_7_7_97608_70964_85407_62090.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_64_63_7_7_97608_70964_85407_62090.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_7_20_0_6_85407_8870_73206_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_7_20_0_6_85407_8870_73206_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_8_21_0_7_97608_8870_85407_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_8_21_0_7_97608_8870_85407_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_9_2_1_0_12201_17740_0_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_0_9_2_1_0_12201_17740_0_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_1_0_0_0_12201_8870_0_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_1_0_0_0_12201_8870_0_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_2_1_0_1_24402_8870_12201_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_2_1_0_1_24402_8870_12201_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_3_2_1_0_12201_17741_0_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_3_2_1_0_12201_17741_0_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_4_3_1_1_24402_17741_12201_8870.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_1_4_3_1_1_24402_17741_12201_8870.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_2_1_0_0_0_6100_4435_0_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_2_1_0_0_0_6100_4435_0_0.JPEG' are the same file
mv: '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_3_1_0_0_0_3050_2217_0_0.JPEG' and '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide4/Slide_4.svs_3_1_0_0_0_3050_2217_0_0.JPEG' are the same file
>> Changing the name of the Slides, from slide5 to slide96
>> Changing the name of all the individual files in slide96
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide96 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 12:56:44 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:44 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 12:56:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 12:56:45 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 12:56:45 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 12:56:45 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 12:56:45 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 12:56:45 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 12:56:45 INFO util.Utils: Successfully started service 'sparkDriver' on port 46178.
17/05/30 12:56:46 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 12:56:46 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 12:56:46 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 12:56:46 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 12:56:46 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-8335528d-c724-49f9-9094-cf592b3bd537
17/05/30 12:56:46 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 12:56:46 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 12:56:46 INFO util.log: Logging initialized @4005ms
17/05/30 12:56:46 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/jobs/job,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/jobs/job/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/stage,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/stage/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/stages/pool,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/stages/pool/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/storage/rdd,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/storage/rdd/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/environment,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/environment/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/executors/threadDump,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/executors/threadDump/json,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/static,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/api,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/jobs/job/kill,null,AVAILABLE}
17/05/30 12:56:46 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/stages/stage/kill,null,AVAILABLE}
17/05/30 12:56:46 INFO server.ServerConnector: Started ServerConnector@27e0f2f5{HTTP/1.1}{0.0.0.0:4040}
17/05/30 12:56:46 INFO server.Server: Started @4298ms
17/05/30 12:56:46 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 12:56:46 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 12:56:46 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:46178/jars/nidan.core-assembly-0.1.jar with timestamp 1496170606886
17/05/30 12:56:47 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 12:56:47 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41391.
17/05/30 12:56:47 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:41391
17/05/30 12:56:47 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 12:56:47 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 41391, None)
17/05/30 12:56:47 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:41391 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 41391, None)
17/05/30 12:56:47 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 41391, None)
17/05/30 12:56:47 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 41391, None)
17/05/30 12:56:47 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@28c88600{/metrics/json,null,AVAILABLE}
17/05/30 12:56:47 WARN root: >>> -11
17/05/30 12:56:47 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:47 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/05/30 12:56:47 WARN root: >> Using 2 partitions in the system
17/05/30 12:56:47 WARN root: >> Generating the SortRDD
17/05/30 12:56:49 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:49 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:49 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:49 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 12:56:50 WARN root: >> SortRDD has been generated with option ZINDEX
17/05/30 12:56:50 WARN root: >> Saving memory by unpersisting RDDs
17/05/30 12:56:50 WARN root: >> Starting to generate the Parquet file
17/05/30 12:56:50 WARN root: >> Storing using GROUP option
17/05/30 12:56:59 WARN datasources.PartitioningAwareFileIndex: The directory hdfs://ctl:9000/nidan/orc/individualORC/slide96 was not found. Was it deleted very recently?
17/05/30 12:57:01 WARN scheduler.TaskSetManager: Stage 3 contains a task of very large size (114119 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:04 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 8/274
17/05/30 12:57:08 WARN scheduler.TaskSetManager: Stage 4 contains a task of very large size (126725 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:11 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 16/274
17/05/30 12:57:14 WARN scheduler.TaskSetManager: Stage 5 contains a task of very large size (114460 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:16 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 24/274
17/05/30 12:57:19 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (129530 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:22 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 32/274
17/05/30 12:57:24 WARN scheduler.TaskSetManager: Stage 7 contains a task of very large size (117866 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:26 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 38/274
17/05/30 12:57:29 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (121223 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:31 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 44/274
17/05/30 12:57:34 WARN scheduler.TaskSetManager: Stage 9 contains a task of very large size (130086 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:35 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 50/274
17/05/30 12:57:37 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (123143 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:39 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 54/274
17/05/30 12:57:41 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (128389 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:43 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 58/274
17/05/30 12:57:45 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (116667 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 62/274
17/05/30 12:57:50 WARN scheduler.TaskSetManager: Stage 13 contains a task of very large size (128850 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 68/274
17/05/30 12:57:56 WARN scheduler.TaskSetManager: Stage 14 contains a task of very large size (127132 KB). The maximum recommended task size is 100 KB.
17/05/30 12:57:57 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 75/274
17/05/30 12:58:00 WARN scheduler.TaskSetManager: Stage 15 contains a task of very large size (129722 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:01 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 79/274
17/05/30 12:58:05 WARN scheduler.TaskSetManager: Stage 16 contains a task of very large size (126036 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:06 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 86/274
17/05/30 12:58:09 WARN scheduler.TaskSetManager: Stage 17 contains a task of very large size (115330 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:11 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 92/274
17/05/30 12:58:13 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (102797 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:14 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 97/274
17/05/30 12:58:17 WARN scheduler.TaskSetManager: Stage 19 contains a task of very large size (97780 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:18 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 100/274
17/05/30 12:58:19 WARN scheduler.TaskSetManager: Stage 20 contains a task of very large size (103510 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 103/274
17/05/30 12:58:22 WARN scheduler.TaskSetManager: Stage 21 contains a task of very large size (100558 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:23 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 106/274
17/05/30 12:58:25 WARN scheduler.TaskSetManager: Stage 22 contains a task of very large size (102366 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:26 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 109/274
17/05/30 12:58:28 WARN scheduler.TaskSetManager: Stage 23 contains a task of very large size (121622 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:30 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 113/274
17/05/30 12:58:32 WARN scheduler.TaskSetManager: Stage 24 contains a task of very large size (117524 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:34 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 119/274
17/05/30 12:58:36 WARN scheduler.TaskSetManager: Stage 25 contains a task of very large size (121897 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:38 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 124/274
17/05/30 12:58:41 WARN scheduler.TaskSetManager: Stage 26 contains a task of very large size (123104 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:43 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 131/274
17/05/30 12:58:46 WARN scheduler.TaskSetManager: Stage 27 contains a task of very large size (128700 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:48 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 138/274
17/05/30 12:58:52 WARN scheduler.TaskSetManager: Stage 28 contains a task of very large size (122770 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:54 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 145/274
17/05/30 12:58:56 WARN scheduler.TaskSetManager: Stage 29 contains a task of very large size (110800 KB). The maximum recommended task size is 100 KB.
17/05/30 12:58:57 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 149/274
17/05/30 12:59:00 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (127539 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:01 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 153/274
17/05/30 12:59:03 WARN scheduler.TaskSetManager: Stage 31 contains a task of very large size (111149 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:04 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 157/274
17/05/30 12:59:07 WARN scheduler.TaskSetManager: Stage 32 contains a task of very large size (117395 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:08 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 161/274
17/05/30 12:59:12 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (117954 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:14 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 169/274
17/05/30 12:59:18 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (124826 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 177/274
17/05/30 12:59:23 WARN scheduler.TaskSetManager: Stage 35 contains a task of very large size (121421 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:24 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 182/274
17/05/30 12:59:28 WARN scheduler.TaskSetManager: Stage 36 contains a task of very large size (123707 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:29 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 188/274
17/05/30 12:59:32 WARN scheduler.TaskSetManager: Stage 37 contains a task of very large size (116918 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:33 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 193/274
17/05/30 12:59:35 WARN scheduler.TaskSetManager: Stage 38 contains a task of very large size (125804 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:36 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 197/274
17/05/30 12:59:39 WARN scheduler.TaskSetManager: Stage 39 contains a task of very large size (116041 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:40 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 202/274
17/05/30 12:59:43 WARN scheduler.TaskSetManager: Stage 40 contains a task of very large size (129760 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:45 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 209/274
17/05/30 12:59:48 WARN scheduler.TaskSetManager: Stage 41 contains a task of very large size (128901 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:50 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 217/274
17/05/30 12:59:53 WARN scheduler.TaskSetManager: Stage 42 contains a task of very large size (112450 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:55 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 224/274
17/05/30 12:59:57 WARN scheduler.TaskSetManager: Stage 43 contains a task of very large size (119900 KB). The maximum recommended task size is 100 KB.
17/05/30 12:59:59 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 230/274
17/05/30 13:00:03 WARN scheduler.TaskSetManager: Stage 44 contains a task of very large size (127563 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:04 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 237/274
17/05/30 13:00:09 WARN scheduler.TaskSetManager: Stage 45 contains a task of very large size (116639 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:11 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 244/274
17/05/30 13:00:15 WARN scheduler.TaskSetManager: Stage 46 contains a task of very large size (121810 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:17 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 252/274
17/05/30 13:00:23 WARN scheduler.TaskSetManager: Stage 47 contains a task of very large size (125916 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:24 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 265/274
17/05/30 13:00:29 WARN scheduler.TaskSetManager: Stage 48 contains a task of very large size (112801 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:30 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 272/274
17/05/30 13:00:32 WARN scheduler.TaskSetManager: Stage 49 contains a task of very large size (26560 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:32 WARN root: 	>> Append to /nidan/orc/individualORC/slide96 Progress: 274/274

real	3m50.974s
user	7m41.660s
sys	0m27.464s
>> Changing the name of all the individual files in slide5
>> Changing the name of the Slides, from slide6 to slide97
>> Changing the name of all the individual files in slide97
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide97 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 13:00:36 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:36 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 13:00:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 13:00:37 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 13:00:37 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 13:00:37 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 13:00:37 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 13:00:37 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 13:00:37 INFO util.Utils: Successfully started service 'sparkDriver' on port 42613.
17/05/30 13:00:37 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 13:00:37 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 13:00:37 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 13:00:37 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 13:00:38 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-fe5f588b-f620-4725-9538-03ab617258fb
17/05/30 13:00:38 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 13:00:38 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 13:00:38 INFO util.log: Logging initialized @3736ms
17/05/30 13:00:38 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/jobs/job,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/jobs/job/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/stage,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/stage/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/stages/pool,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/stages/pool/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/storage/rdd,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/storage/rdd/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/environment,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/environment/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/executors/threadDump,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/executors/threadDump/json,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/static,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/api,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@60cf80e7{/jobs/job/kill,null,AVAILABLE}
17/05/30 13:00:38 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@302fec27{/stages/stage/kill,null,AVAILABLE}
17/05/30 13:00:38 INFO server.ServerConnector: Started ServerConnector@6970ad7d{HTTP/1.1}{0.0.0.0:4040}
17/05/30 13:00:38 INFO server.Server: Started @4008ms
17/05/30 13:00:38 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 13:00:38 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 13:00:38 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:42613/jars/nidan.core-assembly-0.1.jar with timestamp 1496170838686
17/05/30 13:00:38 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 13:00:38 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41035.
17/05/30 13:00:38 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:41035
17/05/30 13:00:38 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 13:00:38 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 41035, None)
17/05/30 13:00:38 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:41035 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 41035, None)
17/05/30 13:00:38 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 41035, None)
17/05/30 13:00:38 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 41035, None)
17/05/30 13:00:39 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@56193c7d{/metrics/json,null,AVAILABLE}
17/05/30 13:00:39 WARN root: >>> -11
17/05/30 13:00:39 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:39 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/05/30 13:00:39 WARN root: >> Using 2 partitions in the system
17/05/30 13:00:39 WARN root: >> Generating the SortRDD
17/05/30 13:00:41 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:41 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:41 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:41 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:00:41 WARN root: >> SortRDD has been generated with option ZINDEX
17/05/30 13:00:41 WARN root: >> Saving memory by unpersisting RDDs
17/05/30 13:00:41 WARN root: >> Starting to generate the Parquet file
17/05/30 13:00:41 WARN root: >> Storing using GROUP option
17/05/30 13:00:49 WARN datasources.PartitioningAwareFileIndex: The directory hdfs://ctl:9000/nidan/orc/individualORC/slide97 was not found. Was it deleted very recently?
17/05/30 13:00:51 WARN scheduler.TaskSetManager: Stage 3 contains a task of very large size (117936 KB). The maximum recommended task size is 100 KB.
17/05/30 13:00:54 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 3/70
17/05/30 13:00:58 WARN scheduler.TaskSetManager: Stage 4 contains a task of very large size (125056 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 6/70
17/05/30 13:01:02 WARN scheduler.TaskSetManager: Stage 5 contains a task of very large size (120060 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:04 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 8/70
17/05/30 13:01:06 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (104110 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:07 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 10/70
17/05/30 13:01:10 WARN scheduler.TaskSetManager: Stage 7 contains a task of very large size (128694 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:12 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 12/70
17/05/30 13:01:14 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (65794 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:14 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 13/70
17/05/30 13:01:16 WARN scheduler.TaskSetManager: Stage 9 contains a task of very large size (74266 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:17 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 14/70
17/05/30 13:01:18 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (77029 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:19 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 15/70
17/05/30 13:01:21 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (109868 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:23 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 17/70
17/05/30 13:01:25 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (98828 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:26 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 19/70
17/05/30 13:01:28 WARN scheduler.TaskSetManager: Stage 13 contains a task of very large size (107286 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:29 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 21/70
17/05/30 13:01:31 WARN scheduler.TaskSetManager: Stage 14 contains a task of very large size (106801 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:32 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 23/70
17/05/30 13:01:34 WARN scheduler.TaskSetManager: Stage 15 contains a task of very large size (129970 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:36 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 25/70
17/05/30 13:01:38 WARN scheduler.TaskSetManager: Stage 16 contains a task of very large size (117385 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:40 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 27/70
17/05/30 13:01:42 WARN scheduler.TaskSetManager: Stage 17 contains a task of very large size (108605 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:43 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 29/70
17/05/30 13:01:46 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (110079 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:47 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 31/70
17/05/30 13:01:49 WARN scheduler.TaskSetManager: Stage 19 contains a task of very large size (99502 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:51 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 33/70
17/05/30 13:01:52 WARN scheduler.TaskSetManager: Stage 20 contains a task of very large size (117773 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:54 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 35/70
17/05/30 13:01:55 WARN scheduler.TaskSetManager: Stage 21 contains a task of very large size (54835 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:56 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 36/70
17/05/30 13:01:57 WARN scheduler.TaskSetManager: Stage 22 contains a task of very large size (94377 KB). The maximum recommended task size is 100 KB.
17/05/30 13:01:58 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 37/70
17/05/30 13:01:59 WARN scheduler.TaskSetManager: Stage 23 contains a task of very large size (93874 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 38/70
17/05/30 13:02:01 WARN scheduler.TaskSetManager: Stage 24 contains a task of very large size (64563 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:02 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 39/70
17/05/30 13:02:04 WARN scheduler.TaskSetManager: Stage 25 contains a task of very large size (126372 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:06 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 41/70
17/05/30 13:02:08 WARN scheduler.TaskSetManager: Stage 26 contains a task of very large size (100289 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:09 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 43/70
17/05/30 13:02:11 WARN scheduler.TaskSetManager: Stage 27 contains a task of very large size (100151 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:12 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 45/70
17/05/30 13:02:14 WARN scheduler.TaskSetManager: Stage 28 contains a task of very large size (100128 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:16 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 47/70
17/05/30 13:02:18 WARN scheduler.TaskSetManager: Stage 29 contains a task of very large size (111784 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:19 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 49/70
17/05/30 13:02:21 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (121861 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:22 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 51/70
17/05/30 13:02:24 WARN scheduler.TaskSetManager: Stage 31 contains a task of very large size (127556 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:27 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 53/70
17/05/30 13:02:28 WARN scheduler.TaskSetManager: Stage 32 contains a task of very large size (110224 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:30 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 55/70
17/05/30 13:02:32 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (102274 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:33 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 57/70
17/05/30 13:02:35 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (96968 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:37 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 59/70
17/05/30 13:02:39 WARN scheduler.TaskSetManager: Stage 35 contains a task of very large size (93063 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:40 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 61/70
17/05/30 13:02:43 WARN scheduler.TaskSetManager: Stage 36 contains a task of very large size (127677 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:45 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 64/70
17/05/30 13:02:47 WARN scheduler.TaskSetManager: Stage 37 contains a task of very large size (98343 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:48 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 66/70
17/05/30 13:02:50 WARN scheduler.TaskSetManager: Stage 38 contains a task of very large size (128321 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:51 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 68/70
17/05/30 13:02:52 WARN scheduler.TaskSetManager: Stage 39 contains a task of very large size (18949 KB). The maximum recommended task size is 100 KB.
17/05/30 13:02:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide97 Progress: 70/70

real	2m18.831s
user	4m26.580s
sys	0m14.296s
>> Changing the name of all the individual files in slide6
>> Changing the name of the Slides, from slide7 to slide98
mv: cannot stat '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide7': No such file or directory
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide98/*.JPEG': No such file or directory
>> Changing the name of all the individual files in slide98
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide98/*.JPEG': No such file or directory
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide98 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 13:02:55 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:02:55 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 13:02:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 13:02:56 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 13:02:56 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 13:02:56 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 13:02:56 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 13:02:56 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 13:02:56 INFO util.Utils: Successfully started service 'sparkDriver' on port 46854.
17/05/30 13:02:56 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 13:02:56 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 13:02:56 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 13:02:56 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 13:02:57 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-347205ce-1cda-408d-9293-97a7cbd6ed2b
17/05/30 13:02:57 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 13:02:57 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 13:02:57 INFO util.log: Logging initialized @3701ms
17/05/30 13:02:57 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 13:02:57 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 13:02:57 INFO server.ServerConnector: Started ServerConnector@69f63d95{HTTP/1.1}{0.0.0.0:4040}
17/05/30 13:02:57 INFO server.Server: Started @3983ms
17/05/30 13:02:57 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 13:02:57 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 13:02:57 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:46854/jars/nidan.core-assembly-0.1.jar with timestamp 1496170977759
17/05/30 13:02:57 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 13:02:57 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35191.
17/05/30 13:02:57 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:35191
17/05/30 13:02:57 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 13:02:57 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 35191, None)
17/05/30 13:02:57 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:35191 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 35191, None)
17/05/30 13:02:57 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 35191, None)
17/05/30 13:02:57 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 35191, None)
17/05/30 13:02:58 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6c8a68c1{/metrics/json,null,AVAILABLE}
17/05/30 13:02:58 WARN root: >>> -11
17/05/30 13:02:58 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:02:58 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
Exception in thread "main" java.lang.NullPointerException
	at scala.collection.mutable.ArrayOps$ofRef$.length$extension(ArrayOps.scala:192)
	at scala.collection.mutable.ArrayOps$ofRef.length(ArrayOps.scala:192)
	at scala.collection.SeqLike$class.size(SeqLike.scala:106)
	at scala.collection.mutable.ArrayOps$ofRef.size(ArrayOps.scala:186)
	at scala.collection.mutable.Builder$class.sizeHint(Builder.scala:69)
	at scala.collection.mutable.ArrayBuilder.sizeHint(ArrayBuilder.scala:22)
	at scala.collection.TraversableLike$class.builder$1(TraversableLike.scala:230)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:233)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)
	at nidan.main.MainOpenSlide$.getMappingIOFiles1(MainOpenSlide.scala:110)
	at nidan.main.MainSparkSQL$.NIDAN_Option11(MainSparkSQL.scala:567)
	at nidan.main.MainSparkSQL$.main(MainSparkSQL.scala:112)
	at nidan.main.MainSparkSQL.main(MainSparkSQL.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

real	0m5.174s
user	0m10.432s
sys	0m0.400s
mv: cannot stat '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide98': No such file or directory
ls: cannot access '/proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256//slide7': No such file or directory
>> Changing the name of all the individual files in slide7
>> Changing the name of the Slides, from slide1 to slide99
>> Changing the name of all the individual files in slide99
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide99 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 13:03:01 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:01 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 13:03:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 13:03:02 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 13:03:02 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 13:03:02 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 13:03:02 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 13:03:02 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 13:03:03 INFO util.Utils: Successfully started service 'sparkDriver' on port 35052.
17/05/30 13:03:03 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 13:03:03 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 13:03:03 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 13:03:03 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 13:03:03 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-a69b23e9-d884-4bef-9894-d71c09345369
17/05/30 13:03:03 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 13:03:03 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 13:03:03 INFO util.log: Logging initialized @3755ms
17/05/30 13:03:03 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 13:03:03 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 13:03:03 INFO server.ServerConnector: Started ServerConnector@693e7a8{HTTP/1.1}{0.0.0.0:4040}
17/05/30 13:03:03 INFO server.Server: Started @4038ms
17/05/30 13:03:03 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 13:03:03 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 13:03:04 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:35052/jars/nidan.core-assembly-0.1.jar with timestamp 1496170984002
17/05/30 13:03:04 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 13:03:04 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46539.
17/05/30 13:03:04 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:46539
17/05/30 13:03:04 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 13:03:04 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 46539, None)
17/05/30 13:03:04 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:46539 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 46539, None)
17/05/30 13:03:04 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 46539, None)
17/05/30 13:03:04 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 46539, None)
17/05/30 13:03:04 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75504cef{/metrics/json,null,AVAILABLE}
17/05/30 13:03:04 WARN root: >>> -11
17/05/30 13:03:04 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:04 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/05/30 13:03:04 WARN root: >> Using 2 partitions in the system
17/05/30 13:03:04 WARN root: >> Generating the SortRDD
17/05/30 13:03:06 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:06 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:06 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:06 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:03:07 WARN root: >> SortRDD has been generated with option ZINDEX
17/05/30 13:03:07 WARN root: >> Saving memory by unpersisting RDDs
17/05/30 13:03:07 WARN root: >> Starting to generate the Parquet file
17/05/30 13:03:07 WARN root: >> Storing using GROUP option
17/05/30 13:03:15 WARN datasources.PartitioningAwareFileIndex: The directory hdfs://ctl:9000/nidan/orc/individualORC/slide99 was not found. Was it deleted very recently?
17/05/30 13:03:17 WARN scheduler.TaskSetManager: Stage 3 contains a task of very large size (112946 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 6/274
17/05/30 13:03:22 WARN scheduler.TaskSetManager: Stage 4 contains a task of very large size (116503 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:23 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 10/274
17/05/30 13:03:26 WARN scheduler.TaskSetManager: Stage 5 contains a task of very large size (120602 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:27 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 14/274
17/05/30 13:03:29 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (100326 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:30 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 18/274
17/05/30 13:03:32 WARN scheduler.TaskSetManager: Stage 7 contains a task of very large size (99155 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:33 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 22/274
17/05/30 13:03:35 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (125817 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:37 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 26/274
17/05/30 13:03:39 WARN scheduler.TaskSetManager: Stage 9 contains a task of very large size (127755 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:41 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 30/274
17/05/30 13:03:43 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (122601 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:45 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 35/274
17/05/30 13:03:47 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (129555 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:49 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 39/274
17/05/30 13:03:51 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (104813 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 43/274
17/05/30 13:03:54 WARN scheduler.TaskSetManager: Stage 13 contains a task of very large size (102651 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:55 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 46/274
17/05/30 13:03:57 WARN scheduler.TaskSetManager: Stage 14 contains a task of very large size (110713 KB). The maximum recommended task size is 100 KB.
17/05/30 13:03:58 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 49/274
17/05/30 13:04:01 WARN scheduler.TaskSetManager: Stage 15 contains a task of very large size (106761 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:02 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 52/274
17/05/30 13:04:05 WARN scheduler.TaskSetManager: Stage 16 contains a task of very large size (127993 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:07 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 57/274
17/05/30 13:04:09 WARN scheduler.TaskSetManager: Stage 17 contains a task of very large size (107377 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:10 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 60/274
17/05/30 13:04:12 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (118408 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:14 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 65/274
17/05/30 13:04:17 WARN scheduler.TaskSetManager: Stage 19 contains a task of very large size (123968 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:18 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 71/274
17/05/30 13:04:21 WARN scheduler.TaskSetManager: Stage 20 contains a task of very large size (112445 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:22 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 76/274
17/05/30 13:04:25 WARN scheduler.TaskSetManager: Stage 21 contains a task of very large size (120793 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:27 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 83/274
17/05/30 13:04:31 WARN scheduler.TaskSetManager: Stage 22 contains a task of very large size (130010 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:33 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 92/274
17/05/30 13:04:36 WARN scheduler.TaskSetManager: Stage 23 contains a task of very large size (108260 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:37 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 98/274
17/05/30 13:04:39 WARN scheduler.TaskSetManager: Stage 24 contains a task of very large size (107469 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:41 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 102/274
17/05/30 13:04:43 WARN scheduler.TaskSetManager: Stage 25 contains a task of very large size (114081 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:44 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 107/274
17/05/30 13:04:47 WARN scheduler.TaskSetManager: Stage 26 contains a task of very large size (120757 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:48 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 112/274
17/05/30 13:04:51 WARN scheduler.TaskSetManager: Stage 27 contains a task of very large size (120307 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 117/274
17/05/30 13:04:55 WARN scheduler.TaskSetManager: Stage 28 contains a task of very large size (114082 KB). The maximum recommended task size is 100 KB.
17/05/30 13:04:56 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 122/274
17/05/30 13:04:59 WARN scheduler.TaskSetManager: Stage 29 contains a task of very large size (119061 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 127/274
17/05/30 13:05:02 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (110777 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:03 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 132/274
17/05/30 13:05:07 WARN scheduler.TaskSetManager: Stage 31 contains a task of very large size (127516 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:09 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 136/274
17/05/30 13:05:12 WARN scheduler.TaskSetManager: Stage 32 contains a task of very large size (96890 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:13 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 141/274
17/05/30 13:05:15 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (98138 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:16 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 144/274
17/05/30 13:05:19 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (112398 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 148/274
17/05/30 13:05:24 WARN scheduler.TaskSetManager: Stage 35 contains a task of very large size (129378 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:25 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 154/274
17/05/30 13:05:28 WARN scheduler.TaskSetManager: Stage 36 contains a task of very large size (116036 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:29 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 159/274
17/05/30 13:05:32 WARN scheduler.TaskSetManager: Stage 37 contains a task of very large size (126108 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:34 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 166/274
17/05/30 13:05:37 WARN scheduler.TaskSetManager: Stage 38 contains a task of very large size (111119 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:38 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 172/274
17/05/30 13:05:41 WARN scheduler.TaskSetManager: Stage 39 contains a task of very large size (122496 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:42 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 178/274
17/05/30 13:05:45 WARN scheduler.TaskSetManager: Stage 40 contains a task of very large size (124249 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 184/274
17/05/30 13:05:49 WARN scheduler.TaskSetManager: Stage 41 contains a task of very large size (124045 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:51 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 190/274
17/05/30 13:05:54 WARN scheduler.TaskSetManager: Stage 42 contains a task of very large size (118600 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:55 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 196/274
17/05/30 13:05:58 WARN scheduler.TaskSetManager: Stage 43 contains a task of very large size (111218 KB). The maximum recommended task size is 100 KB.
17/05/30 13:05:59 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 201/274
17/05/30 13:06:01 WARN scheduler.TaskSetManager: Stage 44 contains a task of very large size (115485 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:03 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 206/274
17/05/30 13:06:06 WARN scheduler.TaskSetManager: Stage 45 contains a task of very large size (111895 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:07 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 211/274
17/05/30 13:06:10 WARN scheduler.TaskSetManager: Stage 46 contains a task of very large size (114628 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:11 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 216/274
17/05/30 13:06:14 WARN scheduler.TaskSetManager: Stage 47 contains a task of very large size (126226 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:15 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 222/274
17/05/30 13:06:18 WARN scheduler.TaskSetManager: Stage 48 contains a task of very large size (118040 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 228/274
17/05/30 13:06:24 WARN scheduler.TaskSetManager: Stage 49 contains a task of very large size (124653 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:25 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 236/274
17/05/30 13:06:30 WARN scheduler.TaskSetManager: Stage 50 contains a task of very large size (119154 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:32 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 244/274
17/05/30 13:06:36 WARN scheduler.TaskSetManager: Stage 51 contains a task of very large size (118659 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:38 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 252/274
17/05/30 13:06:43 WARN scheduler.TaskSetManager: Stage 52 contains a task of very large size (120689 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:44 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 258/274
17/05/30 13:06:47 WARN scheduler.TaskSetManager: Stage 53 contains a task of very large size (121990 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:49 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 262/274
17/05/30 13:06:51 WARN scheduler.TaskSetManager: Stage 54 contains a task of very large size (97318 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:52 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 265/274
17/05/30 13:06:55 WARN scheduler.TaskSetManager: Stage 55 contains a task of very large size (104320 KB). The maximum recommended task size is 100 KB.
17/05/30 13:06:56 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 268/274
17/05/30 13:06:59 WARN scheduler.TaskSetManager: Stage 56 contains a task of very large size (108778 KB). The maximum recommended task size is 100 KB.
17/05/30 13:07:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 271/274
17/05/30 13:07:02 WARN scheduler.TaskSetManager: Stage 57 contains a task of very large size (64678 KB). The maximum recommended task size is 100 KB.
17/05/30 13:07:03 WARN root: 	>> Append to /nidan/orc/individualORC/slide99 Progress: 274/274

real	4m4.916s
user	8m30.516s
sys	0m34.540s
>> Changing the name of all the individual files in slide1
>> Changing the name of the Slides, from slide2 to slide100
>> Changing the name of all the individual files in slide100
>> Append the slides to orc files, Running the command: 
time spark-submit  --name Nidan  --master local[*] --driver-memory 30G  --executor-memory 30G  --executor-cores 8  --num-executors 8  --conf spark.io.compression.codec=lzf  --conf spark.akka.frameSize=1024  --conf spark.driver.maxResultSize=1g  --conf  spark.sql.parquet.compression.codec=uncompressed --class nidan.main.MainSparkSQL ~/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar  -11 /proj/nosql-json-PG0/nidan/KU_IMGS/JPEG256/  /nidan/orc/individualORC/slide100 JPEG 1 ZINDEX GROUP
Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
17/05/30 13:07:07 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:07 INFO spark.SparkContext: Running Spark version 2.1.0
17/05/30 13:07:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/30 13:07:08 INFO spark.SecurityManager: Changing view acls to: dl544
17/05/30 13:07:08 INFO spark.SecurityManager: Changing modify acls to: dl544
17/05/30 13:07:08 INFO spark.SecurityManager: Changing view acls groups to: 
17/05/30 13:07:08 INFO spark.SecurityManager: Changing modify acls groups to: 
17/05/30 13:07:08 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(dl544); groups with view permissions: Set(); users  with modify permissions: Set(dl544); groups with modify permissions: Set()
17/05/30 13:07:09 INFO util.Utils: Successfully started service 'sparkDriver' on port 33948.
17/05/30 13:07:09 INFO spark.SparkEnv: Registering MapOutputTracker
17/05/30 13:07:09 INFO spark.SparkEnv: Registering BlockManagerMaster
17/05/30 13:07:09 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/05/30 13:07:09 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/05/30 13:07:09 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-17c74598-3ad6-4445-b75d-273913f54c31
17/05/30 13:07:09 INFO memory.MemoryStore: MemoryStore started with capacity 15.8 GB
17/05/30 13:07:09 INFO spark.SparkEnv: Registering OutputCommitCoordinator
17/05/30 13:07:09 INFO util.log: Logging initialized @3794ms
17/05/30 13:07:09 INFO server.Server: jetty-9.2.z-SNAPSHOT
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@21694e53{/jobs,null,AVAILABLE}
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@72b16078{/jobs/json,null,AVAILABLE}
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@22c86919{/jobs/job,null,AVAILABLE}
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@70fab835{/jobs/job/json,null,AVAILABLE}
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1b0a7baf{/stages,null,AVAILABLE}
17/05/30 13:07:09 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@62417a16{/stages/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@32057e6{/stages/stage,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@26be6ca7{/stages/stage/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6ea1bcdc{/stages/pool,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@759fad4{/stages/pool/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@64712be{/storage,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@53499d85{/storage/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@30ed9c6c{/storage/rdd,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@782a4fff{/storage/rdd/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@46c670a6{/environment,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@59fc684e{/environment/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae81e1{/executors,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2fd1731c{/executors/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@5ae76500{/executors/threadDump,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@6063d80a{/executors/threadDump/json,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@1133ec6e{/static,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@355e34c7{/,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@54709809{/api,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@2a2da905{/jobs/job/kill,null,AVAILABLE}
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@24f360b2{/stages/stage/kill,null,AVAILABLE}
17/05/30 13:07:10 INFO server.ServerConnector: Started ServerConnector@725e4519{HTTP/1.1}{0.0.0.0:4040}
17/05/30 13:07:10 INFO server.Server: Started @4064ms
17/05/30 13:07:10 INFO util.Utils: Successfully started service 'SparkUI' on port 4040.
17/05/30 13:07:10 INFO ui.SparkUI: Bound SparkUI to 0.0.0.0, and started at http://128.110.152.45:4040
17/05/30 13:07:10 INFO spark.SparkContext: Added JAR file:/users/dl544/orcTrialInfinity/nidan.core/target/scala-2.11/nidan.core-assembly-0.1.jar at spark://128.110.152.45:33948/jars/nidan.core-assembly-0.1.jar with timestamp 1496171230100
17/05/30 13:07:10 INFO executor.Executor: Starting executor ID driver on host localhost
17/05/30 13:07:10 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43188.
17/05/30 13:07:10 INFO netty.NettyBlockTransferService: Server created on 128.110.152.45:43188
17/05/30 13:07:10 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/05/30 13:07:10 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 128.110.152.45, 43188, None)
17/05/30 13:07:10 INFO storage.BlockManagerMasterEndpoint: Registering block manager 128.110.152.45:43188 with 15.8 GB RAM, BlockManagerId(driver, 128.110.152.45, 43188, None)
17/05/30 13:07:10 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 128.110.152.45, 43188, None)
17/05/30 13:07:10 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 128.110.152.45, 43188, None)
17/05/30 13:07:10 INFO handler.ContextHandler: Started o.s.j.s.ServletContextHandler@75504cef{/metrics/json,null,AVAILABLE}
17/05/30 13:07:10 WARN root: >>> -11
17/05/30 13:07:10 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:10 WARN spark.SparkContext: Using an existing SparkContext; some configuration may not take effect.
17/05/30 13:07:10 WARN root: >> Using 2 partitions in the system
17/05/30 13:07:10 WARN root: >> Generating the SortRDD
17/05/30 13:07:12 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:12 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:12 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:12 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/30 13:07:12 WARN root: >> SortRDD has been generated with option ZINDEX
17/05/30 13:07:12 WARN root: >> Saving memory by unpersisting RDDs
17/05/30 13:07:12 WARN root: >> Starting to generate the Parquet file
17/05/30 13:07:12 WARN root: >> Storing using GROUP option
17/05/30 13:07:20 WARN datasources.PartitioningAwareFileIndex: The directory hdfs://ctl:9000/nidan/orc/individualORC/slide100 was not found. Was it deleted very recently?
17/05/30 13:07:22 WARN scheduler.TaskSetManager: Stage 3 contains a task of very large size (108467 KB). The maximum recommended task size is 100 KB.
17/05/30 13:07:26 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 3/70
17/05/30 13:07:29 WARN scheduler.TaskSetManager: Stage 4 contains a task of very large size (123018 KB). The maximum recommended task size is 100 KB.
17/05/30 13:07:31 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 6/70
17/05/30 13:07:33 WARN scheduler.TaskSetManager: Stage 5 contains a task of very large size (111988 KB). The maximum recommended task size is 100 KB.
17/05/30 13:07:35 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 8/70
17/05/30 13:08:00 WARN scheduler.TaskSetManager: Stage 6 contains a task of very large size (121353 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:02 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 10/70
17/05/30 13:08:03 WARN scheduler.TaskSetManager: Stage 7 contains a task of very large size (74021 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:04 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 11/70
17/05/30 13:08:05 WARN scheduler.TaskSetManager: Stage 8 contains a task of very large size (89061 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:06 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 12/70
17/05/30 13:08:07 WARN scheduler.TaskSetManager: Stage 9 contains a task of very large size (80318 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:08 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 13/70
17/05/30 13:08:09 WARN scheduler.TaskSetManager: Stage 10 contains a task of very large size (53734 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:10 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 14/70
17/05/30 13:08:12 WARN scheduler.TaskSetManager: Stage 11 contains a task of very large size (96707 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:13 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 15/70
17/05/30 13:08:15 WARN scheduler.TaskSetManager: Stage 12 contains a task of very large size (91224 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:15 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 16/70
17/05/30 13:08:18 WARN scheduler.TaskSetManager: Stage 13 contains a task of very large size (123623 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 19/70
17/05/30 13:08:23 WARN scheduler.TaskSetManager: Stage 14 contains a task of very large size (129400 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:25 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 22/70
17/05/30 13:08:27 WARN scheduler.TaskSetManager: Stage 15 contains a task of very large size (118194 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:29 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 24/70
17/05/30 13:08:31 WARN scheduler.TaskSetManager: Stage 16 contains a task of very large size (66746 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:31 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 25/70
17/05/30 13:08:33 WARN scheduler.TaskSetManager: Stage 17 contains a task of very large size (88056 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:34 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 26/70
17/05/30 13:08:36 WARN scheduler.TaskSetManager: Stage 18 contains a task of very large size (88490 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:36 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 27/70
17/05/30 13:08:38 WARN scheduler.TaskSetManager: Stage 19 contains a task of very large size (83952 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:39 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 28/70
17/05/30 13:08:40 WARN scheduler.TaskSetManager: Stage 20 contains a task of very large size (88947 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:41 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 29/70
17/05/30 13:08:43 WARN scheduler.TaskSetManager: Stage 21 contains a task of very large size (126001 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:44 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 31/70
17/05/30 13:08:46 WARN scheduler.TaskSetManager: Stage 22 contains a task of very large size (103870 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:47 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 33/70
17/05/30 13:08:49 WARN scheduler.TaskSetManager: Stage 23 contains a task of very large size (113399 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:51 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 35/70
17/05/30 13:08:52 WARN scheduler.TaskSetManager: Stage 24 contains a task of very large size (78224 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:53 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 36/70
17/05/30 13:08:54 WARN scheduler.TaskSetManager: Stage 25 contains a task of very large size (91669 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:55 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 37/70
17/05/30 13:08:57 WARN scheduler.TaskSetManager: Stage 26 contains a task of very large size (73931 KB). The maximum recommended task size is 100 KB.
17/05/30 13:08:57 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 38/70
17/05/30 13:08:59 WARN scheduler.TaskSetManager: Stage 27 contains a task of very large size (84103 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:00 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 39/70
17/05/30 13:09:01 WARN scheduler.TaskSetManager: Stage 28 contains a task of very large size (89958 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:03 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 41/70
17/05/30 13:09:05 WARN scheduler.TaskSetManager: Stage 29 contains a task of very large size (122504 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:08 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 44/70
17/05/30 13:09:09 WARN scheduler.TaskSetManager: Stage 30 contains a task of very large size (125142 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:11 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 46/70
17/05/30 13:09:13 WARN scheduler.TaskSetManager: Stage 31 contains a task of very large size (74927 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:15 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 48/70
17/05/30 13:09:17 WARN scheduler.TaskSetManager: Stage 32 contains a task of very large size (83314 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:17 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 49/70
17/05/30 13:09:19 WARN scheduler.TaskSetManager: Stage 33 contains a task of very large size (76536 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:20 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 50/70
17/05/30 13:09:21 WARN scheduler.TaskSetManager: Stage 34 contains a task of very large size (81744 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:22 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 51/70
17/05/30 13:09:24 WARN scheduler.TaskSetManager: Stage 35 contains a task of very large size (77078 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:24 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 52/70
17/05/30 13:09:26 WARN scheduler.TaskSetManager: Stage 36 contains a task of very large size (121821 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:28 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 54/70
17/05/30 13:09:29 WARN scheduler.TaskSetManager: Stage 37 contains a task of very large size (123735 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:31 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 56/70
17/05/30 13:09:32 WARN scheduler.TaskSetManager: Stage 38 contains a task of very large size (68518 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:33 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 57/70
17/05/30 13:09:34 WARN scheduler.TaskSetManager: Stage 39 contains a task of very large size (118046 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:36 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 59/70
17/05/30 13:09:38 WARN scheduler.TaskSetManager: Stage 40 contains a task of very large size (120828 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:40 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 61/70
17/05/30 13:09:42 WARN scheduler.TaskSetManager: Stage 41 contains a task of very large size (115162 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:44 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 64/70
17/05/30 13:09:45 WARN scheduler.TaskSetManager: Stage 42 contains a task of very large size (64529 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:46 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 65/70
17/05/30 13:09:47 WARN scheduler.TaskSetManager: Stage 43 contains a task of very large size (75813 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:48 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 66/70
17/05/30 13:09:50 WARN scheduler.TaskSetManager: Stage 44 contains a task of very large size (87243 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:50 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 67/70
17/05/30 13:09:52 WARN scheduler.TaskSetManager: Stage 45 contains a task of very large size (121813 KB). The maximum recommended task size is 100 KB.
17/05/30 13:09:54 WARN root: 	>> Append to /nidan/orc/individualORC/slide100 Progress: 70/70

real	2m49.269s
user	4m33.952s
sys	0m14.956s
>> Changing the name of all the individual files in slide2
