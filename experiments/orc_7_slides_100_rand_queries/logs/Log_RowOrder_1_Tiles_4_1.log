Warning: Ignoring non-spark config property: hive.exec.reducers.bytes.per.reducer=67108864
Warning: Ignoring non-spark config property: hive.fetch.task.aggr=false
Warning: Ignoring non-spark config property: hive.merge.sparkfiles=false
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask=true
Warning: Ignoring non-spark config property: hive.merge.size.per.task=256000000
Warning: Ignoring non-spark config property: hive.smbjoin.cache.rows=10000
Warning: Ignoring non-spark config property: hive.merge.smallfiles.avgsize=16000000
Warning: Ignoring non-spark config property: hive.optimize.sort.dynamic.partition=false
Warning: Ignoring non-spark config property: hive.exec.orc.default.stripe.size=67108864
Warning: Ignoring non-spark config property: hive.vectorized.execution.enabled=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication.min.reducer=4
Warning: Ignoring non-spark config property: hive.orc.splits.include.file.footer=false
Warning: Ignoring non-spark config property: hive.merge.mapfiles=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.list-status.num-threads=5
Warning: Ignoring non-spark config property: hive.vectorized.groupby.checkinterval=4096
Warning: Ignoring non-spark config property: hive.compute.query.using.stats=true
Warning: Ignoring non-spark config property: mapreduce.input.fileinputformat.split.maxsize=750000000
Warning: Ignoring non-spark config property: hive.merge.orcfile.stripe.level=true
Warning: Ignoring non-spark config property: hive.auto.convert.join.noconditionaltask.size=894435328
Warning: Ignoring non-spark config property: hive.fetch.task.conversion.threshold=1073741824
Warning: Ignoring non-spark config property: hive.auto.convert.join=true
Warning: Ignoring non-spark config property: hive.optimize.reducededuplication=true
Warning: Ignoring non-spark config property: hive.vectorized.groupby.flush.percent=0.1
Warning: Ignoring non-spark config property: hive.fetch.task.conversion=more
Warning: Ignoring non-spark config property: hive.limit.pushdown.memory.usage=0.4
Warning: Ignoring non-spark config property: hive.vectorized.execution.reduce.enabled=false
Warning: Ignoring non-spark config property: hive.map.aggr=true
Warning: Ignoring non-spark config property: hive.stats.autogather=true
Warning: Ignoring non-spark config property: hive.stats.fetch.column.stats=true
Warning: Ignoring non-spark config property: hive.cbo.enable=true
Warning: Ignoring non-spark config property: hive.map.aggr.hash.percentmemory=0.5
Warning: Ignoring non-spark config property: hive.optimize.index.filter=true
Warning: Ignoring non-spark config property: hive.optimize.bucketmapjoin.sortedmerge=false
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
17/05/22 17:14:48 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/22 17:15:02 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/22 17:15:02 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/22 17:15:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/22 17:15:05 WARN spark.SparkConf: The configuration key 'spark.akka.frameSize' has been deprecated as of Spark 1.6 and may be removed in the future. Please use the new key 'spark.rpc.message.maxSize' instead.
17/05/22 17:15:17 WARN metastore.ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/22 17:15:17 WARN metastore.ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/22 17:15:20 WARN metastore.ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
Spark context Web UI available at http://128.110.152.45:4040
Spark context available as 'sc' (master = spark://ctl:7077, app id = app-20170522171504-0498).
Spark session available as 'spark'.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.0
      /_/
         
Using Scala version 2.11.8 (OpenJDK 64-Bit Server VM, Java 1.8.0_121)
Type in expressions to have them evaluated.
Type :help for more information.

scala> import java.io.File
import java.io.File

scala> import java.io.FileOutputStream
import java.io.FileOutputStream

scala> import org.apache.spark.sql._
import org.apache.spark.sql._

scala> 

scala> val queryMsg = "#QUERY "
queryMsg: String = "#QUERY "

scala> val loadDBMsg = "#LOAD_DB "
loadDBMsg: String = "#LOAD_DB "

scala> val loadTable = "#LOAD_TABLE "
loadTable: String = "#LOAD_TABLE "

scala> val loadsqlHive = "#LOAD_SQL_CONTEXT "
loadsqlHive: String = "#LOAD_SQL_CONTEXT "

scala> val dataSource = "/nidan/orc/individualORC/slide4"
dataSource: String = /nidan/orc/individualORC/slide4

scala> 

scala> def show_timing[T](proc: => T): T = {
     |     val start=System.nanoTime()
     |     val res = proc
     |     val end = System.nanoTime()
     |     println("Time elapsed: " + (end-start)/1000000000.0 + " seconds")
     |     res
     | }
show_timing: [T](proc: => T)T

scala> 

scala> val writeToLocal = (in:(Array[Byte], Long, String)) =>{
     |     val bytes = in._1
     |     val output = in._3
     |     
     |     val writer = new FileOutputStream(output)
     |     writer.write(bytes)
     |     writer.close
     |     1
     |   }
writeToLocal: ((Array[Byte], Long, String)) => Int = <function1>

scala>   
     |   val queries = List(("SELECT imageBytes FROM data WHERE  partitionIndex = 190 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 56 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 46 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 59 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 110 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 97 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 181 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 164 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 90 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 135 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 150 ", 1)
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 26 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 144 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 193 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 232 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 166 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 148 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 121 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 63 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 44 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 57 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 54 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 236 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 152 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 129 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 31 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 111 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 20 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 158 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 163 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 116 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 43 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 77 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 15 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 19 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 180 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 3 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 64 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 195 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 39 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 81 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 17 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 161 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 126 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 134 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 211 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 101 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 151 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 197 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 192 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 12 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 99 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 149 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 60 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 138 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 189 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 69 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 100 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 30 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 186 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 135 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 171 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 120 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 123 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 1 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 167 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 6 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 198 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 178 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 40 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 50 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 228 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 9 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 107 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 141 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 105 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 62 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 155 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 169 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 128 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 35 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 120 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 73 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 27 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 52 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 60 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 200 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 58 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 28 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 136 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 209 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 130 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 231 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 65 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 83 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 8 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 207 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 118 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 37 ", 1),
     | ("SELECT imageBytes FROM data WHERE  partitionIndex = 150 ", 1)
     | )
<console>:40: error: (String, Int) does not take parameters
       ("SELECT imageBytes FROM data WHERE  partitionIndex = 26 ", 1),
       ^

scala> 

scala> val sqlContext = new org.apache.spark.sql.hive.HiveContext(sc)
warning: there was one deprecation warning; re-run with -deprecation for details
sqlContext: org.apache.spark.sql.hive.HiveContext = org.apache.spark.sql.hive.HiveContext@7569bc9f

scala> 

scala> show_timing{sqlContext.read.orc(dataSource).createOrReplaceTempView("data")}
Time elapsed: 3.896511008 seconds

scala> 

scala> show_timing{sqlContext.sql(queries(0)._1).map(_.getAs[Array[Byte]](0)).rdd.zipWithIndex.map{case (bytes, ind ex) => (bytes, index, s"o6_${index}.JPEG")}.collect.map(writeToLocal).filter(_ => false).size}
<console>:36: error: not found: value queries
       show_timing{sqlContext.sql(queries(0)._1).map(_.getAs[Array[Byte]](0)).rdd.zipWithIndex.map{case (bytes, index) => (bytes, index, s"o6_${index}.JPEG")}.collect.map(writeToLocal).filter(_ => false).size}
                                  ^

scala> 

scala> for (query <- queries){
     | println(s">> Running query: ${query._1}")
     | show_timing{sqlContext.sql(query._1).map(_.getAs[Array[Byte]](0)).rdd.zipWithIndex.map{case (bytes, index) = > (bytes, index, s"o6_${index}.JPEG")}.collect.map(writeToLocal).filter(_ => false).size}
     | }
<console>:36: error: not found: value queries
       for (query <- queries){
                     ^

scala> 

scala> :quit

real	0m52.533s
user	2m43.420s
sys	0m2.672s
